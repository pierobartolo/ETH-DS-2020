% -*- root: Main.tex -*-
\section{Autoencoders}
% \subsection*{Kernel SVM}
% \textcolor{red}{TODO: add how to kernelize}
\subsection*{Infomax principle}
$I(X, Y) \doteq H(X) - H(X\vert Y) =$ mutual inform\\
$\theta^* = \argmax_\theta I(X, {enc}_\theta X) = \mathbbm{E}_{Z}[\log(P(X,Z)-\log(P(X)P(Z))]$\\
$\theta^* \simeq \argmax_\theta \sum_i\mathbbm{E}_Z\left[\log p(x_i\vert Z)\right] $\\
It is informative but not Disentangled and Robust
\subsection*{Variation Autoencoders}
Find encoder $q_\phi(z|x)$ and decoder $p_\theta(x|z)$ as: $argmax_{\theta, \phi} \sum_i \log p_{\theta, \phi}(x_i)$: \\

$\log p_{\theta, \phi}(x_i)= E_{z\sim q_{\phi(.|x_i)}} \Big[\log \Big(\frac{p_\theta(x_i, z)}{p_\theta(z | x_i)}\frac{q_\phi(z | x_i)}{q_\phi(z | x_i)}\Big)\Big] = E_{z\sim q_{\phi(.|x_i)}}\Big[\log(\frac{p_\theta(x_i, z)}{q_\phi(z | x_i)})\Big] + E_{z\sim q_{\phi(.|x_i)}}\Big[\log{\frac{q_\phi(z | x_i)}{p_\theta(z|x_i)}}\Big] = E_{z\sim q_{\phi(.|x_i)}} [\log p_\theta(x_i | z)] - D_{KL}(q_\phi(\cdot | x_i) || p(\cdot)) + D_{KL}(q_\phi(\cdot | x_i) || p_\theta(\cdot| x_i))$

First term is Infomax and the second one is a regularization term.
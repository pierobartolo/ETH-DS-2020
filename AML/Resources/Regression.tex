% -*- root: Main.tex -*-
\section{Statistics Recap}
%\subsection*{Linear Regression}
%Error: $\hat{R}(w) = \sum_{i=1}^n (y_i - w^Tx_i)^2 = ||Xw-y||^2_2$\\
%Closed form: $w^*=(X^T X)^{-1} X^T y$\\
%Gradient: $\nabla_w \hat{R}(w) = 2X^T (Xw-y)$
\subsection*{Estimation}
\begin{inparaitem}[\color{red}\textbullet]
\item Consistency: $\hat{\theta_n} \stackrel{\text{\tiny P}}{\rightarrow} \theta$,
i.e. $\forall\epsilon P \{|\hat{\theta_n}-\theta| \geq\epsilon\} \stackrel{n \to\infty}{\longrightarrow} 0 $\\
\item Asymptotic normality: $\sqrt{N}(\theta - \hat{\theta_n}) \to \mathcal{N}(0, J^{-1}IJ^{-1})$ \\
\item Asymptotic efficiency: $\hat{\theta}_n$ minimizes $E[(\hat{\theta}_n-\theta)^2]$ as $n \to \infty$\\
\item Not necessarily efficient for finite samples (e.g.  Stein  estimator  of $\mathcal{N}(\theta,\sigma^2I)$ for $d \geq 3$ is better)\\
\item Equivariant:  if $\hat{\theta}_n$ is the MLE of $\theta$ then $g(\hat{\theta}_n)$ is the MLE of $g(\theta)$ (w/ nice $g$)\\
\item Possibly Biased
\end{inparaitem}

\subsection*{Rao-Cramer}
$\Lambda = \frac{\partial \log \mathbb{P}(x|\theta )}{\partial \theta}$ (score function), $E[\Lambda ]=0$\\
Fisher information: $\mathcal{I}(\theta)= \mathbb{V}[\Lambda]$ \\
$\mathcal{I}(\theta)= E[\Lambda^{2}]= -E[\frac{\partial^2 \log \mathbb{P}(x|\theta ) }{\partial \theta \partial \theta ^{T}}]= -E[\frac{\partial \Lambda}{\partial \theta}]$ \\
\textbf{Oss:} For the whole model:\\ $\mathcal{I}_n = \mathbb{V}\left[\frac{\partial \log \mathbb{P}(x_i, i=1:n|\theta )}{\partial \theta}\right]=n\mathcal{I}$

MSE bound: $E[(\hat \theta_n -\theta )^{2}] \geq \frac{[1 + b^{\prime} (\hat\theta_n)]^{2}}{n E[\Lambda ^{2}]} + {b(\hat\theta_n)}^{2}$ \\
 \\
Cauchy-Schwarz: $|E(XY)|^2 \leq E(X^2) E(Y^2)$ 


% \subsection*{Smoothing Splines}
% $RSS(f,\lambda) = \sum\limits_{i=1}^n (y_i - f(x_i))^2 + \lambda  \int (f''(x))^2dx$\\

%\textbf{Unbiasedness}: $\mathbb{E}[\hat{\beta}] = \mathbb{E}[(X^TX)^{-1}X^Ty] = (X^TX)^{-1}X^T\mathbb{E}[X\beta+\epsilon] = (X^TX)^{-1}(X^TX)\beta+X^T\mathbb{E}[\epsilon] = \beta + 0$
%\textbf{Variance of} $a^T\hat{\beta}$: $\mathbb{V}(a^T(X^TX)^{-1}X^T(X\beta + \epsilon)) = \mathbb{V}(a^T\beta) + \mathbb{E}(a^T(X^TX)^{-1}X^T\epsilon\epsilon^TX(X^TX)^{-1}a) = \sigma^2 a^T(X^TX)^{-1}a$ 

%\subsection*{Gauss-Markov Theorem}
%For any linear estimator $\widetilde{\theta}=c^T\mathbf{y}$ that is unbiased for $a^T\beta$ it holds: $\mathbb{V}(a^T\hat{\beta}) \leq \mathbb{V}(c^T\mathbf{y})$\\
%Proof: Let $c^T \mathbf{y} = a^T\hat{\beta} + a^T\mathbf{D}\mathbf{y} = a^T((\mathbf{X^TX})^{-1}\mathbf{X}^T + \mathbf{D})\mathbf{y}$ be an unbiased estimator of $a^T \beta$; then it follow $a^T \mathbf{DX}\beta = 0$ which implies $\mathbf{DX} = 0$.\\
%$\mathbb{V}(c^T \mathbf{y}) = \mathbb{E}[(c^T \mathbf{y})^2]-\mathbb{E}(c^T \mathbf{y})^2 = c^T(\mathbb{E}\mathbf{y}\mathbf{y}^T - \mathbb{E}\mathbf{y}\mathbb{E}\mathbf{y}^T)c = \sigma^2 c^T c $
%= $\sigma^2 \big( a^T ((\mathbf{X^T X})^{-1}\mathbf{X}^T + \mathbf{D}) (\mathbf{X}(\mathbf{X^T X})^{-1}+\mathbf{D}^T)a \big )$\\
%= $\sigma^2 \big( a^T (\mathbf{X^T X})^{-1}a +\mathbf{DD^T}a \big )$
%= $\mathbb{V}(a^T\hat{\beta}) + a^T \mathbf{DD^T}a \geq \mathbb{V}(a^T\hat{\beta})$ (note: $\mathbf{DD^T}$ is PSD)


%High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\\
%High variance can cause overfitting: modeling the random noise in the training data, rather than the intended outputs.

% \subsection*{Gradient Descent}
% 1. Start arbitrary $w_o \in \mathbb{R}$\\
% 2. For $i$ do $w_{t+1} = w_t - \eta_t \nabla \hat{R}(w_t)$

%\subsection*{Curse of Dimensionality}
%To obtain a reliable estimate at a given regularity, the required number of samples grows exponentially with the dimension of the sample space.

% \subsection*{Expected Error}
% For generalization, minimize the expected error
% $R(w) = \int P(x,y) (y-w^Tx)^2 \partial x \partial y$\\
% $= \mathbb{E}_{x,y}[(y-w^Tx)^2]$
\section{Linear Regression}
\(y = X\beta + \epsilon\) where \(y \in \mathbb{R}^n, \;X \in \mathbb{R}^{n\times d},\;\beta \in \mathbb{R}^d\)

\subsection*{Risk Decomposition Theorem}
\(\mathbb{E}_{Y,D}\left[ \left( Y-\hat{f}(x_0)\right) ^2 \right] = Bias^2 + Var. + Noise\)\\
\(Bias =\left(\mathbb{E}_{D}\left[\hat{f}(x_0)\right] - \mathbb{E}\left[Y\vert X=x_0\right]\right)\)\\
\(Variance =  \mathbb{E}_{D}\left[ \left( \mathbb{E}_{D}\left[\hat{f}(x_0)\right]- \hat{f}(x_0)\right) ^2 \right]\)\\
\(Noise =  \mathbb{E}_{Y}\left[\left(Y-\mathbb{E}\left[Y\vert X=x_0\right]\right)^2\right] \)
\subsection*{Combination of Regression Models:}
$\text{bias}[\hat{f}(x)] = \frac{1}{B} \sum_{i=1}^{B} \text{bias}[\hat{f}_i(x)]$\\
{\scriptsize $\mathbbm{V}[\hat{f}(x)] = \frac{1}{B^2}\sum_i \mathbbm{V}[\hat{f}_i(x)]
+ \frac{1}{B^2}\sum_{i\neq j} cov[\hat{f}_i(x), \hat{f}_j(x)] = \rho \sigma^2 + \frac{1-\rho}{B}\sigma^2 \approx \frac{\sigma^2}{B}$}
\subsection*{Minimum square linear regression}
\(\hat{\beta} = \argmin_{\beta}{\norm{X\beta - y}^2} \Rightarrow \hat{\beta} = \left(X^TX \right)^{-1}X^T y\). Here \(\hat{\beta}\) is the BLUE (Best Linear Unbiased Estimator) \\
Projection of Y on space of X: $X \hat{\beta}$
\subsection*{Lasso regression}
\(\hat{\beta} = \argmin_{\beta}{\norm{X\beta - y}}^2 + \lambda\norm{\beta}_{1}\Rightarrow \hat{\beta} =\)  No closed form (LARS algorithm) but it is a convex problem\\
Bayesian prior: \(p(\beta_i) = \frac{1}{4\sigma^2}exp\left(-\vert\beta_i\vert\frac{\lambda}{2\sigma^2}\right)\)\\
{Const. opt. \scriptsize  $\hat{\beta} = \argmin_{\beta}{\norm{X\beta - y}}^2$ s.t. $\norm{\beta}_1<s_\lambda$}
\subsection*{Ridge regression}
\(\hat{\beta} = \argmin_{\beta}{\norm{X\beta - y}^2} + \lambda\norm{\beta}_{2}^{2}\Rightarrow \hat{\beta}  =  \left(X^TX + \lambda I\right)^{-1}X^T y\) \\
Bayesian prior \(p(\beta) = N(0, \frac{\sigma^2}{\lambda}I)\)\\ 

{Const. opt. \scriptsize  $\hat{\beta} = \argmin_{\beta}{\norm{X\beta - y}^2}$ s.t. $\norm{\beta}_2<s_\lambda$}\\ 

$X\hat{\beta_{ridge}} = X(X^TX + \lambda I)^{-1}X^Ty = UD(D^2 + \lambda I)^{-1}DU^Ty = \sum\limits_{j=1}^{d} u_j \frac{d_j^2}{d_j^2 + \lambda}u_j^Ty$ \\
The shrinkage factor shrinks small singular values and it approaches 1 for large singular values.

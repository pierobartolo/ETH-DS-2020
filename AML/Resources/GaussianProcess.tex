% -*- root: Main.tex -*-
\section{Gaussian Processes}

$p(y_{n+1}|x_{n+1},X, y) = \mathcal{N} (y_{n+1}|\mu_{y_{n+1}}, \sigma_{y_{n+1}}^2)$
\\$\mu_{y_{n+1}} = k^T C_{n}^{-1}y= k^T(K+\sigma^2 I)^{-1}y$
\\$\sigma_{y_{n+1}}^2 = c - k^TC_n^{-1}k$
\\ $C_n = K + \sigma^2I$
\\ $c = k(x_{n+1},x_{n+1}) + \sigma^2$
\\ $k =(x_{n+1},X) \;,  K= (X,X)$



%$p\big(\begin{bmatrix}
%\mathbf{y} \\
%y^*\\
%\end{bmatrix}|x^*,\mathbf{X}, \sigma \big) = \mathcal{N}\big(\begin{bmatrix}
%\mathbf{y} \\
%y^*\\
%\end{bmatrix} | \mathbf{0},\begin{bmatrix}
%\mathbf{C_n} & \mathbf{k} \\
%\mathbf{k}^T & c 
%\end{bmatrix}  \big)$\\
%with $\mathbf{C_n} = \mathbf{K} + \sigma^2 \mathbf{I}, c = k(x_{n+1},x_{n+1}) + \sigma^2,\\
%\mathbf{k}=k(x_{n+1}, \mathbf{X}), \mathbf{K}=k(\mathbf{X}, \mathbf{X})$\\
%$p(y^*|x^*, X, y) = \mathcal{N}(y^*|\mu, \sigma^2)$\\
%with $\mu = k^T C_n^{-1}y, \sigma^2 = c-k^TC_n^{-1}k$\\

%\subsection*{GP Hyperparameter Optimization}
%Log-likelihood:\\
%$l(Y|\theta) = -\frac{n}{2} \log(2\pi) - \frac{1}{2} \log |C_n| - \frac{1}{2} Y^T C_n^{-1}Y$\\
%Set of hyperparameters $\theta$ determine parameters $C_n$. Gradient descent: $\nabla_{\theta_i}l(Y|\theta) = -\frac{1}{2}tr(C_n^{-1} \frac{\partial C_n}{\partial \theta_i}) + \frac{1}{2} Y^T C_n^{-1} \frac{\partial C_n}{\partial \theta_i} C_n^{-1} Y$

\subsection{Kernels}
    A function $k: \mathcal{X} \times \mathcal{X}$ is a kernel if : $k(x_1,x_2) = \big<\phi(x_1),\phi(x_2)\big>$
        
	$k(x,y)$ is a kernel if it's symmetric semidefinite positive:\\
	$\forall \left\{x_1, \dots, x_n \right\}$ then for the Gram Matrix \\
	${\left[K\right]}_{ij}=k(x_i,x_j)$ holds $c^TKc\geq0\forall c$\\
	Closure Properties: \\
    $k(x,y) = k_1(x,y) + k_2(x,y)$, $k(x,y) = k_1(x,y)k_2(x,y)$\\
	$k(x,y) = f(x)f(y)$, $k(x,y) = k_3(\phi(x),\phi(y))$\\
    $k(x,y) = \exp(\alpha k_1(x,y)), \alpha > 0$, $|X \cap Y| = kernel$\\
	$k(x,y) = p(k_1(x,y)), \, p(\cdot)$ \\   
	$k(x,y)=k_1(x,y)/ \sqrt{(k_1(x,x) k_1(y,y)}$\\
	Gaussian (rbf): $k(x,y) = \sigma^2 \exp( -\tfrac{||x-y||^2}{2\l^2})$ \\
	Sigmoid: $k(x,y) = \tanh(k\cdot x^Ty - b)$  \\
	Polynomial: $k(x,y) {=} (x^Ty {+} c)^d$,$d\in N$,$c\geq0$ \\
	Periodic: $k(x,y) = \sigma ^2 exp(-\frac{2\sin ^2 (\pi |x-y|/p)}{\ell ^2})$
	\\Linear: $k(x,y) = \sigma_b^2 + \sigma^2(x-c)(y-c) = xy$\\
	Rational Quadratic: $k(x, y) = \sigma^2(1+\frac{(x-y)^2}{2\alpha l^2})^{-\alpha} \rightarrow RBF$
	\subsection{Kernel Properties}
	$k(u,u) \geq 0 \; \;\forall u$\\
	$k(u,v)^2 \leq k(u,u)k(v,v) \; \; \forall u,v$\\
	$\binom{n+k-1}{k}$ Comb. w rep. $\binom{n+d}{d}$ poly kernel

% \subsubsection*{Polynomial kernel}
% $k_1 = (x^Ty)^m$ represents monomial of deg m \\
% $k_2 = (1+x^Ty)^m$ represents monomials up to deg m

%\subsubsection*{Properties of kernel}
%\begin{inparaitem}
%	\item k must be symmetric\\
%	\item the kernel matrix must be SPD
%\end{inparaitem}

%\subsubsection*{Kernel matrix}
%The kernel matrix $K$ is SPD \\
%$K = 
%\begin{bmatrix}
%k(x_1,x_1) & \dots & k(x_1,x_n) \\
%\vdots & \ddots & \vdots \\
%k(x_n, x_1) & \dots & k(x_n,x_n)
%\end{bmatrix}$\\
%$\left ( XX^T \right )$ for inner product as kernel.

% \subsubsection*{semi-positive-definite matrices}
% $M \in \mathbb{R}^{n\times n}$ is SPD $\Leftrightarrow$\\
% $\forall x \in \mathbb{R}^n: x^TMx \geq 0 \Leftrightarrow$\\
% all eigenvalues of $M$ are positive $\geq 0$

%\subsection*{Nearest Neighbor k-NN}
%$y=sign(\sum \limits_{i=1}^n y_i [x_i \text{ among k nearest neighbors of } x])$

%$k_1(x,y) + k_2(x,y)$ , 
%$k_1(x,y) \cdot k_2(x,y)$, 
%$c \cdot k_1(x,y)$ for $c>0$ , 
%$f(k_1(x,y))$, where $f$ is exponential/polynomial with positive coefficents\\
%$k(x,y) = \phi(x)^T \phi(y)$, for some $\phi$ ass. with k

%\subsubsection*{Parametric vs. Nonparametric}
%\emph{Parametric}: have finite set of parameters\\
%E.g. linear regression, perceptron,...\\
%$f(x) = w^Tx, w\in \mathbb{R}^d$ (d is independent of #data)
% \begin{itemize}
% 	\item[+] computationally not complex
% \end{itemize}

%\emph{Nonparametric}: grows in complexity with the size of the data\\
%E.g. kernelized Perceptron, k-NN,...\\
%$f(x) = \sum_{i=1}^n \alpha_i y_i k(x_i,x_n)$ (depends on #data)\\
% \begin{itemize}
% 	\item[+] potentially much more expressive.
% \end{itemize}
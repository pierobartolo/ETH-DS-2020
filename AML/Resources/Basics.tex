\section{Basics}

\subsection*{Gaussian}
$f(x) = \frac{1}{\sqrt{(2\pi)\sigma^2}} e^{-\frac{1}{2} {\left(\frac{x-\mu}{\sigma}\right)}^2},\quad \mathcal{N}(x|\mu, \sigma^2)$\\
$f(x) = \frac{1}{\sqrt{(2\pi)^d\det\Sigma}} e^{- \frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu)},\quad \mathcal{N}(x|\mu, \Sigma)$\\
$X {\sim} \mathcal{N}(\mu,\Sigma),\;Y{=}A{+}BX \Rightarrow Y{\sim}\mathcal{N}(A{+}B\mu,B\Sigma B^T)$ 
\subsection*{Conditionate Gaussians}
\(
\begin{bmatrix} a_1 \\ a_2 \end{bmatrix} \sim \mathcal{N}\left(\begin{bmatrix}\mu_1 \\ \mu_2\end{bmatrix},\begin{bmatrix} \Sigma_{11}&\Sigma_{12} \\\Sigma_{21}&\Sigma_{22} \end{bmatrix}\right)
\Rightarrow a_2\vert a_1 \sim \mathcal{N}\left(\mu_2 + \Sigma_{21}\Sigma^{-1}_{11}(a_1-\mu_1) , \Sigma_{22}-\Sigma_{21}\Sigma^{-1}_{11}\Sigma_{12}\right)
\)


\subsection*{Primal Dual problem}
Let \(\mathcal{P} = 
	\begin{cases}
		\min_w f(w)\\
		g_i(w)=0\;\forall i\\
		h_j(w)\leq 0\;\forall j\\
	\end{cases}
	\)\\
Then the Slater's condition is:\\
\(\exists w\; \vert g_i(w) = 0, h_j (w) < 0 \;\forall i,j\)\\
The lagrangian is:\\
\(\mathcal{L}(w,\lambda,\alpha)=f(w) + \sum_i\lambda_ig_i(w) + \sum_j \alpha_jh_j(w)\)\\
\(\mathcal{D} = 	
\begin{cases}
	\max_{\lambda,\alpha} \theta(\alpha, \lambda)\\
	\theta(\alpha, \lambda) = \min_w \mathcal{L}(w,\lambda,\alpha)\\
	\alpha_j(w)\geq 0\;\forall j\\
\end{cases}
\)\\
$\mathcal{D}$ is always a convex optimization problem.
In general the solution of the \(\mathcal{D}\) is smaller then $\mathcal{P}$. But if Slater's condition holds then they are equal. And we get the complementary slackness: \(\alpha_j^*h_j(w^*) = 0\;\forall\) \\
 The optimal $w^{*} = min_w {\mathcal{L}(w,\lambda^*,\alpha^*)}$
%General p-norm: $\norm{ x }_p = (\sum_{i=1}^n |x_i|^p)^{1/p}$

\subsection*{Moments}
\begin{inparaitem}[\color{red}\textbullet]
\item $Var[X]=E[XX^T]-E[X]E[X^T]$ \\
\item $Var[X{+}Y]=Var[X]{+}Var[Y]{+}2Cov[X,Y]$ \\
\item $Cov[X,Y] = E[(X - E[X])(Y - E[Y])]$ \\
\item $Cov[aX,bY]{=}abCov[X,Y]$ \\
\end{inparaitem}
\subsection*{Calculus}
\begin{inparaitem}[\color{red}\textbullet]
	\item $\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^\top \mathbf{A}\mathbf{x}) = (\mathbf{A}^\top + \mathbf{A})\mathbf{x} \stackrel{\text{\tiny A sym.}}{=} 2\mathbf{A}\mathbf{x}$ \\
	\item $\frac{\partial}{\partial \mathbf{x}}(\mathbf{b}^\top \mathbf{A}\mathbf{x}) = \mathbf{A}^\top \mathbf{b}$
	\item $\frac{\partial}{\partial \mathbf{X}}(\mathbf{c}^\top \mathbf{X} \mathbf{b}) = \mathbf{c}\mathbf{b}^\top$ \\
	\item $x^T A x = Tr(x^T A x) = Tr(x x^T A) = Tr(A x x^T)$ \\
	\item $\tfrac{\partial}{\partial A} Tr(AB) {=} B^T$
	\item $\frac{\partial}{\partial A}|A| {=}|A| A^{-T}$ \\
	\item $\sigma(x) = \frac{1}{1+e^{-x}}$ \\
	\item $\nabla \sigma(x) = \sigma(x)(1-\sigma(x)) = \sigma(x)\sigma(-x)$\\
	\item $\nabla \text{tanh}(x) = 1-\text{tanh}^2(x)$ \\
	\item $tanhx {=} \frac{sinhx}{coshx} {=} \frac{e^{x}-e^{-x}}{e^{x} + e^{x}}$
	\item $2\sigma(x)-1 = tanh(x/2)$
	\item $f(x) \sim f(x_0) + (x-x_0)^T \nabla_f(x_0) + \dfrac{1}{2}(x-x_0)^T H_f(x_0)(x-x_0)$
\end{inparaitem}
\subsection*{Newton's Method}
$x^{(n+1)} \gets x^{(n)}-H^{-1}_F(x^{(n)})\nabla_{F}(x^{(n)})$\\
$f(x^*)=0, f'(x^*) \neq 0 \implies Q$\\
$f(x^*)=0,f^{(k)}(x^*)=0 \implies L$\\
$x^{(n+1)} \gets x^{(n)}-kH^{-1}_F(x^{(n)})\nabla_{F}(x^{(n)}) \implies Q$



\subsection*{Jensen's inequality}
	$\varphi$: convex  $\rightarrow$ $\varphi(\mathbb{E}[X]) \leq \mathbb{E}[\varphi(X)]$
